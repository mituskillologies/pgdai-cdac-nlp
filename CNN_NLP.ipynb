{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLrjnFPuquE4"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nTUhx5PXhipl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAN732jCqxwe"
      },
      "outputs": [],
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r' )\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxHbya3lq1fl"
      },
      "outputs": [],
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile( '[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub( '' , w) for w in tokens]\n",
        "  # filter out tokens not in vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  tokens = ' ' .join(tokens)\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FqX0S-bq4Sg"
      },
      "outputs": [],
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  documents = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if is_train and filename.startswith( 'cv9' ):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith( 'cv9' ):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load the doc\n",
        "    doc = load_doc(path)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc, vocab)\n",
        "    # add to list\n",
        "    documents.append(tokens)\n",
        "  return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9xYdBKrq-RT"
      },
      "outputs": [],
      "source": [
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "  # load documents\n",
        "  neg = process_docs( 'drive/MyDrive/review_polarity/txt_sentoken/neg' , vocab, is_train)\n",
        "  pos = process_docs( 'drive/MyDrive/review_polarity/txt_sentoken/pos' , vocab, is_train)\n",
        "  docs = neg + pos\n",
        "  # prepare labels\n",
        "  labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "  return docs, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZNTx4TvrCFY"
      },
      "outputs": [],
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIEwRPe0rE_2"
      },
      "outputs": [],
      "source": [
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "  # integer encode\n",
        "  encoded = tokenizer.texts_to_sequences(docs)\n",
        "  # pad sequences\n",
        "  padded = pad_sequences(encoded, maxlen=max_length, padding= 'post' )\n",
        "  return padded"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "def define_model(vocab_size, max_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "  model.add(Conv1D(filters=32, kernel_size=8, activation= 'relu' ))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(10, activation= 'relu' ))\n",
        "  model.add(Dense(1, activation= 'sigmoid' ))\n",
        "  # compile network\n",
        "  model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy'  ])\n",
        "  # summarize defined model\n",
        "  model.summary()\n",
        "  plot_model(model, to_file= 'model.png' , show_shapes=True)\n",
        "  return model"
      ],
      "metadata": {
        "id": "HEESBRVLtqsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgF4HcwvrHuO"
      },
      "outputs": [],
      "source": [
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
        "  # clean review\n",
        "  line = clean_doc(review, vocab)\n",
        "  # encode and pad review\n",
        "  padded = encode_docs(tokenizer, max_length, [line])\n",
        "  # predict sentiment\n",
        "  yhat = model.predict(padded, verbose=0)\n",
        "  # retrieve predicted percentage and label\n",
        "  percent_pos = yhat[0,0]\n",
        "  if round(percent_pos) == 0:\n",
        "    return (1-percent_pos), 'NEGATIVE'\n",
        "  return percent_pos, 'POSITIVE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAjQBRifrLcS"
      },
      "outputs": [],
      "source": [
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTyMGxxUrP5b"
      },
      "outputs": [],
      "source": [
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AXfGVLRrR96"
      },
      "outputs": [],
      "source": [
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0bftSK6rUPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63b5639-65b4-4698-f0a1-759787dee70a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Vocabulary size: 25768 \n"
          ]
        }
      ],
      "source": [
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print( ' Vocabulary size: %d ' % vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjpi8mcCrWY3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "879b3e98-24c6-4796-a43d-dd06df8d56c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Maximum length: 1317 \n"
          ]
        }
      ],
      "source": [
        "# calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print( ' Maximum length: %d ' % max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opbRD5AKrY9e"
      },
      "outputs": [],
      "source": [
        "# encode data\n",
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
        "Xtest = encode_docs(tokenizer, max_length, test_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDURT3lZrbCI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c479de6-4caa-4bb4-c217-277446dc41ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1317, 100)         2576800   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 1310, 32)          25632     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 655, 32)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 20960)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                209610    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,812,053\n",
            "Trainable params: 2,812,053\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "57/57 [==============================] - 17s 282ms/step - loss: 0.6910 - accuracy: 0.5494\n",
            "Epoch 2/10\n",
            "57/57 [==============================] - 16s 280ms/step - loss: 0.5239 - accuracy: 0.7689\n",
            "Epoch 3/10\n",
            "57/57 [==============================] - 16s 280ms/step - loss: 0.1102 - accuracy: 0.9722\n",
            "Epoch 4/10\n",
            "57/57 [==============================] - 21s 362ms/step - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "57/57 [==============================] - 16s 280ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "57/57 [==============================] - 16s 281ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "57/57 [==============================] - 16s 281ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "57/57 [==============================] - 16s 280ms/step - loss: 9.5546e-04 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "57/57 [==============================] - 16s 280ms/step - loss: 7.7810e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "57/57 [==============================] - 19s 328ms/step - loss: 6.5654e-04 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc4d6fb24c0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# define model\n",
        "model = define_model(vocab_size, max_length)\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10)\n",
        "\n",
        "# load the model\n",
        "# model = load_model( 'model.h5' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMBcFllJrfie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41cf78dc-6643-40a3-ad44-8c0e500f7b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Train Accuracy: 100.00 \n"
          ]
        }
      ],
      "source": [
        "# evaluate model on training dataset\n",
        "_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
        "print( ' Train Accuracy: %.2f ' % (acc*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbZrx0fSrhoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e141c44-cdb2-4e3b-c01d-ca7c3aae2b7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Test Accuracy: 88.50 \n"
          ]
        }
      ],
      "source": [
        "# evaluate model on test dataset\n",
        "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print( ' Test Accuracy: %.2f ' % (acc*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwRUGe7vrmiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa3ff7d-8b24-43d9-c92d-624cf7e8bf3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
            "Sentiment: POSITIVE (52.044%) \n"
          ]
        }
      ],
      "source": [
        "# test positive text\n",
        "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
        "percent, sentiment = predict_sentiment(text)\n",
        "print( 'Review: [%s]\\nSentiment: %s (%.3f%%) ' % (text, sentiment, percent*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZrExMqIrteW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14a5a439-412e-4c34-e004-2f3bc373325b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Review: [This is a bad movie. Do not watch it. It sucks.]\n",
            "Sentiment: NEGATIVE (54.083%) \n"
          ]
        }
      ],
      "source": [
        "# test negative text\n",
        "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
        "percent, sentiment = predict_sentiment(text)\n",
        "print( ' Review: [%s]\\nSentiment: %s (%.3f%%) ' % (text, sentiment, percent*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yr4TE767wiZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}