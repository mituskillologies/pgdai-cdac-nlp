{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5TEqB6v1fyi"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r' )\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text"
      ],
      "metadata": {
        "id": "2h5t5-go1lUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "  # split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "  # prepare regex for char filtering\n",
        "  re_punc = re.compile( '[%s]' % re.escape(string.punctuation))\n",
        "  # remove punctuation from each word\n",
        "  tokens = [re_punc.sub( '' , w) for w in tokens]\n",
        "  # filter out tokens not in vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  tokens = ' ' .join(tokens)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "fDLxE6vl2Y_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())"
      ],
      "metadata": {
        "id": "Tk4RR0Wb8vUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load all docs in a directory\n",
        "def process_train(directory, vocab):\n",
        "  documents = list()\n",
        "  for filename in listdir(directory):\n",
        "    if not filename.startswith( 'cv9' ):\n",
        "      path = directory + '/' + filename\n",
        "      doc = load_doc(path)\n",
        "      tokens = clean_doc(doc, vocab)\n",
        "      documents.append(tokens)\n",
        "  return documents\n",
        "\n",
        "def process_test(directory, vocab):\n",
        "  documents = list()\n",
        "  for filename in listdir(directory):\n",
        "    if filename.startswith( 'cv9' ):\n",
        "      path = directory + '/' + filename\n",
        "      doc = load_doc(path)\n",
        "      tokens = clean_doc(doc, vocab)\n",
        "      documents.append(tokens)\n",
        "  return documents  "
      ],
      "metadata": {
        "id": "TNNfe65J5mYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  documents = list()\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "    if is_train and filename.startswith( 'cv9' ):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith( 'cv9' ):\n",
        "      continue\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load the doc\n",
        "    doc = load_doc(path)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc, vocab)\n",
        "    # add to list\n",
        "    documents.append(tokens)\n",
        "  return documents"
      ],
      "metadata": {
        "id": "c6C9cQBIANX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "  # load documents\n",
        "  neg = process_docs('drive/MyDrive/review_polarity/txt_sentoken/neg',vocab, is_train)\n",
        "  pos = process_docs('drive/MyDrive/review_polarity/txt_sentoken/pos',vocab, is_train)\n",
        "  docs = neg + pos\n",
        "  # prepare labels\n",
        "  labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "  return docs, labels"
      ],
      "metadata": {
        "id": "wIHtKD1M53HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "8wC-Mtrb83Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "  # integer encode\n",
        "  encoded = tokenizer.texts_to_sequences(docs)\n",
        "  # pad sequences\n",
        "  padded = pad_sequences(encoded, maxlen=max_length, padding= 'post' )\n",
        "  return padded"
      ],
      "metadata": {
        "id": "UclxHnUODyxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "def define_model(vocab_size, max_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "  model.add(Conv1D(filters=32, kernel_size=8, activation= 'relu' ))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(10, activation= 'relu' ))\n",
        "  model.add(Dense(1, activation= 'sigmoid' ))\n",
        "  # compile network\n",
        "  model.compile(loss= 'binary_crossentropy',optimizer= 'adam',metrics=['accuracy'])\n",
        "  # summarize defined model\n",
        "  model.summary()\n",
        "  plot_model(model, to_file= 'model.png' , show_shapes=True)\n",
        "  return model"
      ],
      "metadata": {
        "id": "RLMauCInD1Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the vocabulary\n",
        "vocab = load_doc('vocab.txt')\n",
        "vocab = set(vocab.split())"
      ],
      "metadata": {
        "id": "uyiAESLxOzeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)"
      ],
      "metadata": {
        "id": "VyKvSV11RD58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)"
      ],
      "metadata": {
        "id": "n3Jpy693RyYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print( 'Vocabulary size: %d ' % vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sBjfKWNRy3M",
        "outputId": "94848bb4-ca10-46ce-c4b2-eba820fe1910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Vocabulary size: 25768 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print( ' Maximum length: %d ' % max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUVYLB3dR5M2",
        "outputId": "b7c777e3-f67b-4e29-b07e-0197ce64d754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Maximum length: 1317 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode data\n",
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
        "Xtest = encode_docs(tokenizer, max_length, test_docs)"
      ],
      "metadata": {
        "id": "IYHy-uXpTDsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = define_model(vocab_size, max_length)\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJyYMqosU1XI",
        "outputId": "4721b17a-e1c8-4f9d-c0a4-ec7f1ce801f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 1317, 100)         2576800   \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 1310, 32)          25632     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 655, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 20960)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                209610    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,812,053\n",
            "Trainable params: 2,812,053\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "180/180 [==============================] - 12s 64ms/step - loss: 0.6761 - accuracy: 0.5033\n",
            "Epoch 2/10\n",
            "180/180 [==============================] - 12s 68ms/step - loss: 0.3035 - accuracy: 0.8606\n",
            "Epoch 3/10\n",
            "180/180 [==============================] - 11s 63ms/step - loss: 0.0294 - accuracy: 0.9933\n",
            "Epoch 4/10\n",
            "180/180 [==============================] - 12s 65ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "180/180 [==============================] - 11s 63ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "180/180 [==============================] - 11s 63ms/step - loss: 9.1585e-04 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "180/180 [==============================] - 11s 62ms/step - loss: 6.5209e-04 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "180/180 [==============================] - 11s 63ms/step - loss: 4.9994e-04 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "180/180 [==============================] - 11s 62ms/step - loss: 3.9580e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "180/180 [==============================] - 11s 62ms/step - loss: 3.1975e-04 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9664654ca0>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model on training dataset\n",
        "_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
        "print( ' Train Accuracy: %.2f ' % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhafLss_XgDi",
        "outputId": "9e78f4a4-1276-458e-e0ad-988df259d3c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Train Accuracy: 100.00 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model on testing dataset\n",
        "_, acc = model.evaluate(Xtest, ytest, verbose=0, batch_size=1)\n",
        "print( ' Test Accuracy: %.2f ' % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju1jtN0DXi6u",
        "outputId": "f7883696-c346-4afa-f93b-4aaa79aaa1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Test Accuracy: 87.50 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review):\n",
        "  # clean review\n",
        "  line = clean_doc(review, vocab)\n",
        "  # encode and pad review\n",
        "  padded = encode_docs(tokenizer, max_length, [line])\n",
        "  # predict sentiment\n",
        "  yhat = model.predict(padded, verbose=0)\n",
        "  # retrieve predicted percentage and label\n",
        "  percent_pos = yhat[0,0]\n",
        "  if round(percent_pos) == 0:\n",
        "    return (1-percent_pos), 'NEGATIVE'\n",
        "  return percent_pos, 'POSITIVE'"
      ],
      "metadata": {
        "id": "PGzLno2LWIlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test positive text\n",
        "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
        "percent, sentiment = predict_sentiment(text)\n",
        "print( 'Review: [%s]\\nSentiment: %s (%.3f%%) ' % (text, sentiment, percent*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0c44-7IXWs4",
        "outputId": "4a0a93fc-e1b7-4d6b-f012-3f82f9aa584d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
            "Sentiment: NEGATIVE (60.848%) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eNXRagtxbBE2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}